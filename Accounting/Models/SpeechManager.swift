import Foundation
import Speech
import AVFoundation
import SwiftUI
import Observation

/// è¯­éŸ³è¯†åˆ«ç®¡ç†å™¨
@MainActor
@Observable
final class SpeechManager: NSObject {
    // MARK: - Published Properties
    
    /// å®æ—¶è½¬å½•æ–‡æœ¬
    var transcript: String = ""
    
    /// æ˜¯å¦æ­£åœ¨å½•éŸ³
    var isRecording: Bool = false
    
    /// éŸ³é¢‘çº§åˆ«ï¼ˆ0.0 - 1.0ï¼Œç”¨äºæ³¢å½¢æ˜¾ç¤ºï¼‰
    var audioLevel: Float = 0.0
    
    /// é”™è¯¯ä¿¡æ¯
    var errorMessage: String?
    
    /// æƒé™çŠ¶æ€
    var authorizationStatus: SFSpeechRecognizerAuthorizationStatus = .notDetermined
    var microphonePermissionStatus: AVAudioSession.RecordPermission = .undetermined
    
    // MARK: - Private Properties
    
    private let speechRecognizer: SFSpeechRecognizer?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    // audioEngine æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œæ ‡è®°ä¸º nonisolated(unsafe) ä»¥åœ¨ deinit ä¸­ä½¿ç”¨
    nonisolated(unsafe) private let audioEngine = AVAudioEngine()
    private let audioSession = AVAudioSession.sharedInstance()
    
    // ç”¨äº deinit æ¸…ç†çš„æ ‡è®°ï¼ˆééš”ç¦»ï¼Œç”¨äºå®‰å…¨è®¿é—®ï¼‰
    private nonisolated(unsafe) var needsCleanup: Bool = false
    
    // MARK: - Initialization
    
    override init() {
        // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨ï¼Œä¼˜å…ˆä½¿ç”¨ä¸­æ–‡ï¼ˆå¿…é¡»åœ¨ super.init() ä¹‹å‰ï¼‰
        let recognizer: SFSpeechRecognizer?
        if let cnRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN")) {
            recognizer = cnRecognizer
        } else {
            // å¦‚æœä¸­æ–‡ä¸å¯ç”¨ï¼Œä½¿ç”¨å½“å‰è¯­è¨€ç¯å¢ƒ
            recognizer = SFSpeechRecognizer()
        }
        
        // åˆå§‹åŒ–æ‰€æœ‰å­˜å‚¨å±æ€§
        self.speechRecognizer = recognizer
        
        // è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super.init()
        
        // æ£€æŸ¥æƒé™çŠ¶æ€
        authorizationStatus = SFSpeechRecognizer.authorizationStatus()
        microphonePermissionStatus = audioSession.recordPermission
        
        // é…ç½®è¯­éŸ³è¯†åˆ«å™¨
        speechRecognizer?.delegate = self
    }
    
    // MARK: - Permission Management
    
    /// è¯·æ±‚æ‰€æœ‰å¿…éœ€çš„æƒé™
    /// - Returns: æ˜¯å¦å·²æˆäºˆæ‰€æœ‰æƒé™
    func requestPermissions() async -> Bool {
        // è¯·æ±‚è¯­éŸ³è¯†åˆ«æƒé™
        let speechStatus = await requestSpeechAuthorization()
        
        // è¯·æ±‚éº¦å…‹é£æƒé™
        let microphoneStatus = await requestMicrophonePermission()
        
        return speechStatus == .authorized && microphoneStatus == .granted
    }
    
    /// è¯·æ±‚è¯­éŸ³è¯†åˆ«æƒé™
    @discardableResult
    private func requestSpeechAuthorization() async -> SFSpeechRecognizerAuthorizationStatus {
        let currentStatus = SFSpeechRecognizer.authorizationStatus()
        
        guard currentStatus == .notDetermined else {
            authorizationStatus = currentStatus
            return currentStatus
        }
        
        return await withCheckedContinuation { continuation in
            SFSpeechRecognizer.requestAuthorization { status in
                Task { @MainActor in
                    self.authorizationStatus = status
                    continuation.resume(returning: status)
                }
            }
        }
    }
    
    /// è¯·æ±‚éº¦å…‹é£æƒé™
    @discardableResult
    private func requestMicrophonePermission() async -> AVAudioSession.RecordPermission {
        let currentStatus = audioSession.recordPermission
        
        guard currentStatus == .undetermined else {
            microphonePermissionStatus = currentStatus
            return currentStatus
        }
        
        return await withCheckedContinuation { continuation in
            audioSession.requestRecordPermission { granted in
                Task { @MainActor in
                    let status: AVAudioSession.RecordPermission = granted ? .granted : .denied
                    self.microphonePermissionStatus = status
                    continuation.resume(returning: status)
                }
            }
        }
    }
    
    /// æ£€æŸ¥æ˜¯å¦å·²æˆäºˆæ‰€æœ‰æƒé™
    var hasAllPermissions: Bool {
        authorizationStatus == .authorized && microphonePermissionStatus == .granted
    }
    
    // MARK: - Recording Control
    
    /// å¼€å§‹å½•éŸ³
    func startRecording() throws {
        // æ£€æŸ¥æƒé™
        guard hasAllPermissions else {
            throw SpeechError.permissionDenied
        }
        
        // æ£€æŸ¥è¯­éŸ³è¯†åˆ«å™¨æ˜¯å¦å¯ç”¨
        guard let recognizer = speechRecognizer, recognizer.isAvailable else {
            throw SpeechError.recognizerUnavailable
        }
        
        // å¦‚æœæ­£åœ¨å½•éŸ³ï¼Œå…ˆåœæ­¢
        if isRecording {
            stopRecording()
        }
        
        // é‡ç½®çŠ¶æ€
        transcript = ""
        errorMessage = nil
        
        // é…ç½®éŸ³é¢‘ä¼šè¯
        try configureAudioSession()
        
        // åˆ›å»ºè¯†åˆ«è¯·æ±‚
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let request = recognitionRequest else {
            throw SpeechError.requestCreationFailed
        }
        
        // é…ç½®è¯†åˆ«è¯·æ±‚
        request.shouldReportPartialResults = true
        
        // è·å–éŸ³é¢‘è¾“å…¥èŠ‚ç‚¹
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        // å®‰è£…éŸ³é¢‘å¼•æ“çš„ tap æ¥ç›‘å¬éŸ³é¢‘
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak self] buffer, _ in
            request.append(buffer)
            
            // æ›´æ–°éŸ³é¢‘çº§åˆ«ï¼ˆç”¨äºæ³¢å½¢æ˜¾ç¤ºï¼‰
            if let channelData = buffer.floatChannelData {
                let channelDataValue = channelData.pointee
                let channelDataValueArray = stride(from: 0, to: Int(buffer.frameLength), by: buffer.stride)
                    .map { channelDataValue[$0] }
                
                let rms = sqrt(channelDataValueArray.map { $0 * $0 }.reduce(0, +) / Float(buffer.frameLength))
                let avgPower = 20 * log10(rms)
                let normalizedLevel = max(0.0, min(1.0, (avgPower + 60) / 60)) // å½’ä¸€åŒ–åˆ° 0-1
                
                Task { @MainActor in
                    self?.audioLevel = normalizedLevel
                }
            }
        }
        
        // å‡†å¤‡å¹¶å¯åŠ¨éŸ³é¢‘å¼•æ“
        audioEngine.prepare()
        try audioEngine.start()
        
        // å¼€å§‹è¯†åˆ«ä»»åŠ¡
        recognitionTask = recognizer.recognitionTask(with: request) { [weak self] result, error in
            Task { @MainActor in
                guard let self = self else { return }
                
                if let error = error {
                    self.handleRecognitionError(error)
                    return
                }
                
                if let result = result {
                    // æ›´æ–°è½¬å½•æ–‡æœ¬
                    self.transcript = result.bestTranscription.formattedString
                    
                    // å¦‚æœè¯†åˆ«å®Œæˆï¼ˆæœ€ç»ˆç»“æœï¼‰
                    if result.isFinal {
                        self.stopRecording()
                    }
                }
            }
        }
        
        isRecording = true
        needsCleanup = true
        print("ğŸ¤ [SpeechManager] å¼€å§‹å½•éŸ³")
    }
    
    /// åœæ­¢å½•éŸ³
    func stopRecording() {
        guard isRecording else { return }
        
        // åœæ­¢éŸ³é¢‘å¼•æ“
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        
        // å®Œæˆè¯†åˆ«è¯·æ±‚
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        
        // å–æ¶ˆè¯†åˆ«ä»»åŠ¡
        recognitionTask?.cancel()
        recognitionTask = nil
        
        // é‡ç½®éŸ³é¢‘çº§åˆ«
        audioLevel = 0.0
        
        isRecording = false
        needsCleanup = false
        print("ğŸ›‘ [SpeechManager] åœæ­¢å½•éŸ³")
    }
    
    // MARK: - Audio Session Configuration
    
    /// é…ç½®éŸ³é¢‘ä¼šè¯
    private func configureAudioSession() throws {
        try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
        try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
    }
    
    // MARK: - Error Handling
    
    /// å¤„ç†è¯†åˆ«é”™è¯¯
    private func handleRecognitionError(_ error: Error) {
        if let speechError = error as? SpeechError {
            errorMessage = speechError.localizedDescription
        } else {
            let nsError = error as NSError
            
            switch nsError.code {
            case 216: // SFSpeechRecognizerErrorCode.notAvailable
                errorMessage = "è¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨"
            case 201: // SFSpeechRecognizerErrorCode.recognitionTaskUnavailable
                errorMessage = "è¯†åˆ«ä»»åŠ¡ä¸å¯ç”¨"
            case 1700: // SFSpeechRecognizerErrorCode.audioEngineUnavailable
                errorMessage = "éŸ³é¢‘å¼•æ“ä¸å¯ç”¨"
            case 1701: // SFSpeechRecognizerErrorCode.networkUnavailable
                errorMessage = "ç½‘ç»œä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œè¯­éŸ³è¯†åˆ«"
            default:
                errorMessage = "è¯­éŸ³è¯†åˆ«é”™è¯¯: \(error.localizedDescription)"
            }
        }
        
        print("âŒ [SpeechManager] è¯†åˆ«é”™è¯¯: \(error.localizedDescription)")
        stopRecording()
    }
    
    // MARK: - Cleanup
    
    /// ééš”ç¦»çš„æ¸…ç†æ–¹æ³•ï¼Œç”¨äº deinit
    nonisolated private func performCleanup() {
        // åªæ¸…ç†éŸ³é¢‘å¼•æ“ï¼Œè¿™æ˜¯çº¿ç¨‹å®‰å…¨çš„
        if needsCleanup {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }
    }
    
    deinit {
        // åœ¨ deinit ä¸­è°ƒç”¨ééš”ç¦»çš„æ¸…ç†æ–¹æ³•
        performCleanup()
    }
}

// MARK: - SFSpeechRecognizerDelegate

extension SpeechManager: SFSpeechRecognizerDelegate {
    nonisolated func speechRecognizer(_ speechRecognizer: SFSpeechRecognizer, availabilityDidChange available: Bool) {
        Task { @MainActor [weak self] in
            guard let self = self else { return }
            if !available && self.isRecording {
                self.errorMessage = "è¯­éŸ³è¯†åˆ«æœåŠ¡å·²ä¸å¯ç”¨"
                self.stopRecording()
            }
        }
    }
}

// MARK: - SpeechError

enum SpeechError: LocalizedError {
    case permissionDenied
    case recognizerUnavailable
    case requestCreationFailed
    case audioSessionConfigurationFailed
    case audioEngineStartFailed
    
    var errorDescription: String? {
        switch self {
        case .permissionDenied:
            return "éœ€è¦æˆäºˆè¯­éŸ³è¯†åˆ«å’Œéº¦å…‹é£æƒé™æ‰èƒ½ä½¿ç”¨æ­¤åŠŸèƒ½"
        case .recognizerUnavailable:
            return "è¯­éŸ³è¯†åˆ«æœåŠ¡å½“å‰ä¸å¯ç”¨"
        case .requestCreationFailed:
            return "æ— æ³•åˆ›å»ºè¯†åˆ«è¯·æ±‚"
        case .audioSessionConfigurationFailed:
            return "éŸ³é¢‘ä¼šè¯é…ç½®å¤±è´¥"
        case .audioEngineStartFailed:
            return "éŸ³é¢‘å¼•æ“å¯åŠ¨å¤±è´¥"
        }
    }
}
